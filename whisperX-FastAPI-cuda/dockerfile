## Verwenden Sie das neueres PyTorch-Image mit cuDNN 8 (bessere Kompatibilität mit WhisperX 3.4.2)
FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-devel

# Alternative: NVIDIA CUDA Image (falls PyTorch-Image nicht verfügbar)
#FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

ENV PYTHON_VERSION=3.10
# Stellen Sie sicher, dass sowohl die CUDA- als auch die cuDNN-Bibliotheken im Pfad sind
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/include:/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH

# AWS-spezifische Mirror-Konfiguration hinzufügen
RUN sed -i 's/archive.ubuntu.com/eu-central-1.ec2.archive.ubuntu.com/g' /etc/apt/sources.list && \
    sed -i 's/security.ubuntu.com/eu-central-1.ec2.archive.ubuntu.com/g' /etc/apt/sources.list

# Install dependencies including build tools for PyAV compilation
# PyAV (Abhängigkeit von WhisperX 3.4.2) benötigt pkg-config und FFmpeg-Entwicklungsbibliotheken
RUN export DEBIAN_FRONTEND=noninteractive \
    && apt-get -y update \
    && apt-get -y install --no-install-recommends \
    git \
    ffmpeg \
    pkg-config \
    build-essential \
    libavformat-dev \
    libavcodec-dev \
    libavdevice-dev \
    libavutil-dev \
    libswscale-dev \
    libswresample-dev \
    libavfilter-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Copy application code
COPY app app/
COPY tests tests/
COPY app/gunicorn_logging.conf .
COPY requirements requirements/

# Überprüfen und anzeigen der CUDA-Bibliotheken und ihrer Pfade für Diagnosezwecke
# cuDNN 8 sollte jetzt verfügbar sein (libcudnn_ops_infer.so.8)
RUN find /usr -name "libcudnn*.so*" | sort

# PyTorch ist bereits im Image installiert, aber wir müssen die Versionen fixieren
# für Kompatibilität mit pyannote.audio (benötigt AudioMetaData in torchaudio 2.8.x)
# Upgrade pip und setuptools für bessere Kompatibilität
RUN pip install --upgrade pip setuptools wheel \
    && pip install --no-cache-dir -r requirements/system.txt \
    && pip install --no-cache-dir -r requirements/prod.txt \
    # Clean pip cache and temporary files
    && rm -rf /root/.cache /tmp/*

# Explizite Tests auf CUDA-Verfügbarkeit
RUN python -c "import torch; print('CUDA available:', torch.cuda.is_available()); print('PyTorch version:', torch.__version__); print('CUDA version:', torch.version.cuda if torch.cuda.is_available() else 'NA')"

EXPOSE 8000

# Erhöhen Sie die Anzahl der Worker, wenn Sie genug GPUs und Speicher haben
ENTRYPOINT ["gunicorn", "--bind", "0.0.0.0:8000", "--workers", "1", "--timeout", "0", "--log-config", "gunicorn_logging.conf", "app.main:app", "-k", "uvicorn.workers.UvicornWorker"]