## Verwenden Sie das PyTorch-Image (verfügbar und funktioniert)
FROM pytorch/pytorch:2.5.1-cuda12.1-cudnn9-runtime

# Alternative: NVIDIA CUDA Image (falls PyTorch-Image nicht verfügbar)
#FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

ENV PYTHON_VERSION=3.11
# Stellen Sie sicher, dass sowohl die CUDA- als auch die cuDNN-Bibliotheken im Pfad sind
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/include:/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH

# AWS-spezifische Mirror-Konfiguration hinzufügen
RUN sed -i 's/archive.ubuntu.com/eu-central-1.ec2.archive.ubuntu.com/g' /etc/apt/sources.list && \
    sed -i 's/security.ubuntu.com/eu-central-1.ec2.archive.ubuntu.com/g' /etc/apt/sources.list

# Install dependencies and clean up in the same layer
RUN export DEBIAN_FRONTEND=noninteractive \
    && apt-get -y update \
    && apt-get -y install --no-install-recommends \
    software-properties-common \
    && add-apt-repository ppa:deadsnakes/ppa \
    && apt-get -y update \
    && apt-get -y install --no-install-recommends \
    python${PYTHON_VERSION} \
    python${PYTHON_VERSION}-dev \
    python${PYTHON_VERSION}-distutils \
    python3-pip \
    git \
    ffmpeg \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* \
    && ln -s -f /usr/bin/python${PYTHON_VERSION} /usr/bin/python3 \
    && ln -s -f /usr/bin/python${PYTHON_VERSION} /usr/bin/python

# UV Package Manager entfernt - verwende Standard pip

WORKDIR /app

# Copy application code
COPY app app/
COPY tests tests/
COPY app/gunicorn_logging.conf .
COPY requirements requirements/

# Überprüfen und anzeigen der CUDA-Bibliotheken und ihrer Pfade für Diagnosezwecke
RUN find /usr -name "libcudnn*.so*" | sort

# Ändern Sie die PyTorch-Installation zu einer Version, die mit CUDA 11.8 und cuDNN 8 kompatibel ist
# PyTorch ist bereits im Image installiert, nur Dependencies installieren
# Verwende die gleiche Strategie wie der funktionierende CPU-Container
RUN pip install --no-cache-dir -r requirements/prod.txt \
    # Clean pip cache and temporary files
    && rm -rf /root/.cache /tmp/*

# Explizite Tests auf CUDA-Verfügbarkeit
RUN python -c "import torch; print('CUDA available:', torch.cuda.is_available()); print('PyTorch version:', torch.__version__); print('CUDA version:', torch.version.cuda if torch.cuda.is_available() else 'NA')"

EXPOSE 8000

# Erhöhen Sie die Anzahl der Worker, wenn Sie genug GPUs und Speicher haben
ENTRYPOINT ["gunicorn", "--bind", "0.0.0.0:8000", "--workers", "1", "--timeout", "0", "--log-config", "gunicorn_logging.conf", "app.main:app", "-k", "uvicorn.workers.UvicornWorker"]